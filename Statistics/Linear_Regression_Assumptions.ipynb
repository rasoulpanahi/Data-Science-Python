{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8a3b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "<div style=\"background-color: #00008B; padding: 20px;\">\n",
    "    <h1 style=\"font-size: 100px; color: #ffffff;\">Linear Regression Assumptions</h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2c956e",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4b0082; padding: 15px; border-radius: 10px; background-color: #f3f3ff;\">\n",
    "<h2 style=\"color: #4b0082;\">Assumptions of Linear Regression</h2>\n",
    "\n",
    "<p>Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. For linear regression to produce reliable results, several key assumptions must be met:</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">1. Linearity</h3>\n",
    "<p>The relationship between the dependent variable and the independent variables should be linear. This means that changes in the independent variables should result in proportional changes in the dependent variable.</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">2. Independence</h3>\n",
    "<p>Observations should be independent of each other. This implies that the residuals (errors) are not correlated across observations. Independence is crucial for the validity of the regression model.</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">3. Homoscedasticity</h3>\n",
    "<p>The residuals should have constant variance at every level of the independent variables. This assumption ensures that the spread or \"scatter\" of the residuals is consistent across all levels of the independent variables.</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">4. Normality of Residuals</h3>\n",
    "<p>The residuals (errors) of the model should be approximately normally distributed. This assumption is particularly important for hypothesis testing and constructing confidence intervals.</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">5. No Multicollinearity</h3>\n",
    "<p>The independent variables should not be too highly correlated with each other. High correlation between independent variables can lead to multicollinearity, which makes it difficult to assess the individual effect of each variable.</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Summary</h3>\n",
    "<p>For a linear regression model to provide valid and reliable results, it is essential that these assumptions are met. Linearity, independence, homoscedasticity, normality of residuals, and no multicollinearity are the key assumptions that ensure the effectiveness and accuracy of the linear regression analysis.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85893164",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4b0082; padding: 15px; border-radius: 10px; background-color: #f9f9f9;\">\n",
    "\n",
    "<h1 style=\"color: #4b0082;\">1.Linearity</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; color: #333;\">\n",
    "The linearity assumption in linear regression states that there should be a linear relationship between the independent variables and the dependent variable. This means that the change in the dependent variable is proportional to the change in the independent variables. Linearity ensures that the model accurately captures the underlying relationship in the data.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Context and Importance</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "When the linearity assumption holds, the linear regression model can provide reliable and interpretable results. However, if the relationship between the independent and dependent variables is not linear, the model may fail to capture the true pattern, leading to biased estimates and poor predictions.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Checking for Linearity</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "To check for linearity, consider the following methods:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Scatter Plots:</strong> Plotting the dependent variable against each independent variable can help visualize the relationships. A linear relationship will show a roughly straight-line pattern.</li>\n",
    "<li><strong>Residual Plots:</strong> Plotting the residuals against the predicted values can reveal deviations from linearity. For a linear model, the residuals should be randomly scattered around zero without any discernible pattern.</li>\n",
    "<li><strong>Correlation Coefficients:</strong> Calculating the Pearson correlation coefficient between the independent and dependent variables can provide an indication of the linear relationship's strength.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Addressing Non-Linearity</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "If you detect non-linearity, consider these approaches to address it:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Transforming Variables:</strong> Applying transformations (e.g., log, square root, polynomial terms) to the independent or dependent variables can help linearize the relationship.</li>\n",
    "<li><strong>Adding Polynomial Terms:</strong> Including polynomial terms (e.g., $x^2$, $x^3$) can model non-linear relationships within the framework of linear regression.</li>\n",
    "<li><strong>Using Non-Linear Models:</strong> If the relationship is highly non-linear, consider using non-linear models such as decision trees, neural networks, or other machine learning algorithms.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Example of Scatter Plot</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Below is an example of a scatter plot showing a linear relationship between an independent variable and the dependent variable:\n",
    "</p>\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_correlation-regression/Graph-BMI-Cholesterol.png\" alt=\"Linear Relationship Scatter Plot\" width=\"60%\" style=\"border: 1px solid #ccc;\">\n",
    "</p>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "In contrast, a scatter plot showing a non-linear relationship would display a curved pattern, indicating that the linearity assumption is violated.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7072fa5b",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #000000; padding: 15px; border-radius: 10px; background-color: #f9f9f9;\">\n",
    "<h2 style=\"color: #4b0082;\">Linearity in Linear Regression</h2>\n",
    "\n",
    "<p>Linearity is a fundamental assumption in linear regression that specifies the relationship between the dependent variable $ y $ and the independent variable(s) $ x $ is linear. This means that the change in $ y $ due to a unit change in $ x $ is constant, and can be represented by a linear equation:</p>\n",
    "\n",
    "<p style=\"text-align: center;\"><strong>$ y = \\beta_0 + \\beta_1 x + \\epsilon $</strong></p>\n",
    "\n",
    "<p>In this equation:</p>\n",
    "<ul>\n",
    "<li>$ y $ is the dependent variable.</li>\n",
    "<li>$ x $ is the independent variable.</li>\n",
    "<li>$ \\beta_0 $ is the y-intercept (the value of $ y $ when $ x = 0 $).</li>\n",
    "<li>$ \\beta_1 $ is the slope of the line (the change in $ y $ for a one-unit change in $ x $).</li>\n",
    "<li>$ \\epsilon $ is the error term (the difference between the observed and predicted values of $ y $).</li>\n",
    "</ul>\n",
    "\n",
    "<p>The assumption of linearity implies that the relationship between $ x $ and $ y $ can be accurately described by this linear equation.</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Why Linearity is Important</h3>\n",
    "<p>Ensuring the linearity assumption is met is crucial because it affects the model's ability to accurately predict the dependent variable. If the relationship between the variables is not linear, the model may produce biased estimates and unreliable predictions.</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Checking for Linearity</h3>\n",
    "<p>To check for linearity, we can use scatter plots and residual plots:</p>\n",
    "<ul>\n",
    "<li><strong>Scatter Plot:</strong> Plotting $ y $ against $ x $. If the data points form a straight line, the linearity assumption is likely satisfied.</li>\n",
    "<li><strong>Residual Plot:</strong> Plotting the residuals (errors) against the predicted values. If the residuals display a random pattern, the linearity assumption is likely met.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Here's an example of a scatter plot that illustrates a linear relationship:</p>\n",
    "<div style=\"text-align: center;\">\n",
    "<img src=\"https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/bs704_correlation-regression/Graph-BMI-Cholesterol.png\" alt=\"Scatter plot with linear regression line\" width=\"400\">\n",
    "<p style=\"color: #4b0082;\"><em>Scatter plot with a linear regression line</em></p>\n",
    "</div>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Non-Linearity</h3>\n",
    "<p>If the relationship between the variables is non-linear, transformations of the independent variables (e.g., logarithmic, polynomial) or using non-linear models may be necessary to capture the relationship accurately.</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Mathematical Representation</h3>\n",
    "<p>The multiple linear regression model extends the simple linear model to multiple predictors:</p>\n",
    "<p style=\"text-align: center;\"><strong>$ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_n x_n + \\epsilon $</strong></p>\n",
    "\n",
    "<p>In this equation:</p>\n",
    "<ul>\n",
    "<li>$ x_1, x_2, \\ldots, x_n $ are the independent variables.</li>\n",
    "<li>$ \\beta_1, \\beta_2, \\ldots, \\beta_n $ are the coefficients representing the change in $ y $ for a one-unit change in the respective $ x $ variable.</li>\n",
    "</ul>\n",
    "\n",
    "<p>Linearity in this context means that the effect of each predictor on $ y $ is additive and constant.</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Summary</h3>\n",
    "<p>Linearity is a key assumption in linear regression, ensuring a straight-line relationship between the dependent and independent variables. Checking for linearity using scatter plots and residual plots helps validate this assumption, ensuring reliable model predictions. When non-linearity is present, alternative modeling approaches may be required.</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0555d01",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4b0082; padding: 15px; border-radius: 10px; background-color: #f9f9f9;\">\n",
    "\n",
    "<h1 style=\"color: #4b0082;\">2.Independence</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; color: #333;\">\n",
    "The independence assumption in linear regression states that the observations should be independent of each other. This means that the residuals (errors) for one observation should not be correlated with the residuals for any other observation. Independence is crucial because it ensures that the information in one observation does not provide information about another, allowing for valid inferences from the model.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Context and Importance</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "In many real-world scenarios, data may be collected in a way that introduces dependencies between observations. For example, time series data often have a temporal structure where past values influence future values. Similarly, data collected from clusters or groups (like students within the same school) can exhibit dependencies.\n",
    "</p>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "When the independence assumption is violated, the standard errors of the estimated coefficients can be incorrect, leading to misleading hypothesis tests and confidence intervals. Therefore, checking for independence is a crucial step in validating a linear regression model.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">How to Check for Independence</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "One common method to check for independence is by examining the residuals of the regression model:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Durbin-Watson Test:</strong> This statistical test detects the presence of autocorrelation (a specific kind of correlation between consecutive residuals). A value close to 2 indicates no autocorrelation, while values approaching 0 or 4 suggest positive or negative autocorrelation, respectively.</li>\n",
    "<li><strong>Residual Plots:</strong> Plotting residuals against time (for time series data) or the order of observations can help identify patterns that suggest dependency. Ideally, residuals should appear random and show no obvious patterns.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Addressing Independence Violations</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "If you find evidence of dependency in your data, there are several approaches to address it:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Time Series Models:</strong> For time-dependent data, consider using models designed for time series analysis, such as ARIMA or exponential smoothing models.</li>\n",
    "<li><strong>Mixed-Effects Models:</strong> For data with group structures, mixed-effects models can account for both fixed and random effects, capturing the dependencies within groups.</li>\n",
    "<li><strong>Data Transformation:</strong> Sometimes, transforming the data can help mitigate dependency issues. For instance, differencing time series data can help remove trends and seasonality.</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c057007d",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #ff0000; padding: 15px; border-radius: 10px; background-color: #ffe5e5; margin-top: 20px;\">\n",
    "<h3 style=\"color: #ff0000;\"><span style=\"font-size: 18px; color: #ff0000; padding-right: 5px;\">&#9888;</span> Independence of Predictor Variables</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "It is important to note that the independence assumption in linear regression does not refer to the independence between the predictor variables (independent variables). Instead, it focuses on the independence of the residuals. While multicollinearity (high correlation between predictor variables) can be a concern in linear regression, it is a separate issue from the independence of residuals.\n",
    "</p>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Multicollinearity can be addressed by checking the Variance Inflation Factor (VIF) and using techniques such as principal component analysis (PCA) or ridge regression if necessary.\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd7efac",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #000000; padding: 15px; border-radius: 10px; background-color: #f9f9f9;\">\n",
    "<h2 style=\"color: #800080;\">Independence in Linear Regression</h2>\n",
    "\n",
    "\n",
    "<h3 style=\"color: #800080;\">Independence</h3>\n",
    "<p style=\"font-size: 16px; color: #333;\">\n",
    "The independence assumption in linear regression states that the residuals (errors) should be independent. This means that the error term of one observation should not be correlated with the error term of another observation. Independence of residuals is crucial for the validity of statistical tests and confidence intervals.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Why Independence is Important</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Independence is essential because if the residuals are correlated, it indicates that there is some pattern left in the data that the model has not captured. This can lead to underestimated standard errors and inflated t-statistics, resulting in incorrect inferences about the significance of predictors.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Checking for Independence</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "To check for independence, you can use:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Durbin-Watson Test:</strong> This test checks for the presence of autocorrelation in the residuals. Values close to 2 indicate no autocorrelation, while values deviating significantly from 2 suggest positive or negative autocorrelation.</li>\n",
    "<li><strong>Plotting Residuals:</strong> A plot of residuals over time or against fitted values can reveal patterns. A random scatter suggests independence, while systematic patterns indicate autocorrelation.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Non-Independence</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Non-independence of residuals often occurs in time series data, where observations are collected over time. This leads to autocorrelation, where residuals are correlated with their own past values. It can also occur in clustered data, where residuals within clusters are more similar to each other than to those in different clusters.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Mathematical Representation</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Let $ y_i $ be the observed value and $ \\hat{y}_i $ be the predicted value. The residual for the $ i $-th observation is:\n",
    "</p>\n",
    "<p style=\"text-align: center; font-size: 16px;\">\n",
    "$$ e_i = y_i - \\hat{y}_i $$\n",
    "</p>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Independence assumes that for any two residuals $ e_i $ and $ e_j $:\n",
    "</p>\n",
    "<p style=\"text-align: center; font-size: 16px;\">\n",
    "$$ \\text{Cov}(e_i, e_j) = 0 \\quad \\text{for} \\quad i \\neq j $$\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Summary</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Ensuring the independence of residuals is crucial for the validity of linear regression models. Non-independence can lead to incorrect inferences and predictions. By conducting appropriate tests and visualizations, you can check for independence and address issues when they arise.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">How to Address Non-Independence</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "If you detect non-independence, consider these approaches:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Time Series Models:</strong> Use models like ARIMA that are designed for time series data with autocorrelation.</li>\n",
    "<li><strong>Mixed Models:</strong> Use mixed-effects models to account for clustered or grouped data.</li>\n",
    "<li><strong>Include Lagged Variables:</strong> For time series data, include lagged variables to capture autocorrelation.</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1497f1f3",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4b0082; padding: 15px; border-radius: 10px; background-color: #f9f9f9;\">\n",
    "\n",
    "<h1 style=\"color: #4b0082;\">3.Homoscedasticity</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; color: #333;\">\n",
    "The homoscedasticity assumption in linear regression refers to the requirement that the residuals (errors) have constant variance across all levels of the independent variables. In other words, the spread or \"scatter\" of the residuals should be roughly the same for all predicted values. This assumption ensures that the model's predictions are equally reliable across the entire range of data.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Context and Importance</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "When the assumption of homoscedasticity is met, the residuals are evenly distributed, and the model's accuracy is consistent. However, if this assumption is violated (a condition known as heteroscedasticity), the residuals exhibit non-constant variance. This can lead to inefficient estimates and biased standard errors, which affect the reliability of hypothesis tests and confidence intervals.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Checking for Homoscedasticity</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "To check for homoscedasticity, consider the following methods:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Residual Plots:</strong> Plotting the residuals against the predicted values or the independent variables can help identify patterns. In a homoscedastic model, the residuals should be randomly scattered without forming any clear pattern.</li>\n",
    "<li><strong>Breusch-Pagan Test:</strong> This statistical test checks for heteroscedasticity by testing whether the variance of the residuals is dependent on the values of the independent variables.</li>\n",
    "<li><strong>White Test:</strong> Another statistical test for detecting heteroscedasticity that does not assume a specific form of heteroscedasticity.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Addressing Heteroscedasticity</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "If you detect heteroscedasticity, consider these approaches to address it:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Transforming the Dependent Variable:</strong> Applying a transformation to the dependent variable (e.g., log, square root) can help stabilize the variance of the residuals.</li>\n",
    "<li><strong>Weighted Least Squares (WLS):</strong> This method assigns weights to the observations based on the variance of the residuals, giving less weight to observations with higher variance.</li>\n",
    "<li><strong>Robust Standard Errors:</strong> Using robust standard errors (also known as heteroscedasticity-consistent standard errors) can provide valid standard errors even when heteroscedasticity is present.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Example of Residual Plot</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Below is an example of a residual plot that shows homoscedasticity, where the residuals are evenly scattered around the horizontal axis:\n",
    "</p>\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"https://sphweb.bumc.bu.edu/otlt/MPH-Modules/BS/R/R5_Correlation-Regression/Illustration%20of%20Simple%20Linear%20Regression.gif\" alt=\"Homoscedasticity Residual Plot\" width=\"60%\" style=\"border: 1px solid #ccc;\">\n",
    "</p>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "In contrast, a plot showing heteroscedasticity might have a funnel shape, indicating that the variance of the residuals changes with the level of the predicted values.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2147d476",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #000000; padding: 15px; border-radius: 10px; background-color: #f9f9f9;\">\n",
    "<h2 style=\"color: #800080;\">Homoscedasticity in Linear Regression</h2>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Homoscedasticity</h3>\n",
    "<p style=\"font-size: 16px; color: #333;\">\n",
    "Homoscedasticity, also known as homogeneity of variance, refers to the assumption that the variance of the residuals (errors) is constant across all levels of the independent variables. This means that the spread of the residuals should be roughly the same across the range of predicted values.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Why Homoscedasticity is Important</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Homoscedasticity is crucial because heteroscedasticity (the opposite of homoscedasticity) can lead to inefficient estimates and biased standard errors, which in turn affects hypothesis tests and confidence intervals. Ensuring homoscedasticity helps maintain the validity of statistical inferences.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Checking for Homoscedasticity</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "To check for homoscedasticity, you can use:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Residual Plots:</strong> Plot the residuals against the fitted values. If the residuals are randomly scattered around zero with a constant spread, homoscedasticity is present.</li>\n",
    "<li><strong>Breusch-Pagan Test:</strong> This statistical test checks for heteroscedasticity. A significant result indicates the presence of heteroscedasticity.</li>\n",
    "<li><strong>White's Test:</strong> Another test that can detect heteroscedasticity by examining whether the variance of the residuals is dependent on the values of the independent variables.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Non-Homoscedasticity</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "When residuals display a pattern, such as funneling (widening or narrowing) or clustering, it indicates heteroscedasticity. This suggests that the model's errors vary at different levels of the independent variables, which can undermine the reliability of the regression results.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Mathematical Representation</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Let $ e_i $ be the residual for the $ i $-th observation. Homoscedasticity assumes that the variance of the residuals is constant:\n",
    "</p>\n",
    "<p style=\"text-align: center; font-size: 16px;\">\n",
    "$$ \\text{Var}(e_i) = \\sigma^2 \\quad \\text{for all} \\quad i $$\n",
    "</p>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Where $ \\sigma^2 $ is a constant.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Summary</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Homoscedasticity ensures that the residuals have a constant variance, which is crucial for reliable statistical inference in linear regression. Detecting and addressing heteroscedasticity helps in maintaining the validity and efficiency of the model.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">How to Address Non-Homoscedasticity</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "If you detect heteroscedasticity, consider these approaches:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Transforming Variables:</strong> Apply transformations (e.g., log, square root) to the dependent variable to stabilize variance.</li>\n",
    "<li><strong>Weighted Least Squares:</strong> Use weighted regression to give less weight to observations with higher variance.</li>\n",
    "<li><strong>Robust Standard Errors:</strong> Adjust the standard errors to be robust to heteroscedasticity.</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9158dc1",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4b0082; padding: 15px; border-radius: 10px; background-color: #f9f9f9;\">\n",
    "\n",
    "<h1 style=\"color: #4b0082;\">4.Normality of Residuals</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; color: #333;\">\n",
    "The normality of residuals assumption in linear regression states that the residuals (errors) of the model should be approximately normally distributed. This assumption is particularly important for constructing valid confidence intervals and conducting hypothesis tests, as many statistical tests rely on the normality assumption.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Context and Importance</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "When the residuals are normally distributed, the linear regression model produces unbiased estimates and reliable confidence intervals and hypothesis tests. However, if the residuals deviate significantly from normality, it can lead to incorrect conclusions and reduced efficiency in parameter estimation.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Checking for Normality</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "To check for normality of residuals, consider the following methods:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Histogram:</strong> Plotting a histogram of the residuals can provide a visual check for normality. A roughly bell-shaped histogram indicates normality.</li>\n",
    "<li><strong>Q-Q Plot:</strong> A quantile-quantile (Q-Q) plot compares the distribution of the residuals to a normal distribution. If the points fall approximately along a straight line, the residuals are normally distributed.</li>\n",
    "<li><strong>Shapiro-Wilk Test:</strong> A statistical test that evaluates the null hypothesis that the residuals are normally distributed. A high p-value suggests normality.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Addressing Non-Normality</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "If you detect non-normality, consider these approaches to address it:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Transforming Variables:</strong> Applying transformations (e.g., log, square root) to the dependent variable can sometimes normalize the residuals.</li>\n",
    "<li><strong>Adding Polynomial or Interaction Terms:</strong> Including additional terms in the model can help account for non-linear relationships that may be causing non-normal residuals.</li>\n",
    "<li><strong>Using Robust Regression Methods:</strong> Techniques such as quantile regression can provide valid inference even when the residuals are not normally distributed.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Example of Q-Q Plot</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Below is an example of a Q-Q plot showing that the residuals are normally distributed:\n",
    "</p>\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"https://sphweb.bumc.bu.edu/otlt/mph-modules/bs/r/r2_summarystats-graphs/QQ-Plot-normal.png\" alt=\"Normality Q-Q Plot\" width=\"60%\" style=\"border: 1px solid #ccc;\">\n",
    "</p>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "In contrast, a Q-Q plot showing non-normal residuals would display points that deviate significantly from the straight line.\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabb322b",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #000000; padding: 15px; border-radius: 10px; background-color: #f9f9f9;\">\n",
    "<h2 style=\"color: #800080;\">Normality of Residuals in Linear Regression</h2>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Normality of Residuals</h3>\n",
    "<p style=\"font-size: 16px; color: #333;\">\n",
    "The normality of residuals assumption states that the residuals (errors) of the regression model should be normally distributed. This assumption is particularly important for small sample sizes, as it ensures the validity of confidence intervals and hypothesis tests.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Why Normality is Important</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Normality of residuals is crucial for making reliable inferences about the population parameters. If the residuals are normally distributed, it justifies the use of parametric tests that rely on the normality assumption, ensuring accurate p-values and confidence intervals.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Checking for Normality</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "To check for normality, you can use:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Q-Q Plot:</strong> A quantile-quantile plot compares the quantiles of the residuals with the quantiles of a normal distribution. If the points lie approximately along a straight line, the residuals are normally distributed.</li>\n",
    "<li><strong>Histogram:</strong> A histogram of the residuals can show their distribution. A bell-shaped curve indicates normality.</li>\n",
    "<li><strong>Shapiro-Wilk Test:</strong> This statistical test checks the null hypothesis that the data was drawn from a normal distribution. A non-significant result (p > 0.05) indicates normality.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Non-Normality</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "When residuals deviate from normality, it indicates potential issues such as outliers, skewness, or kurtosis. This can affect the reliability of statistical tests and confidence intervals, leading to inaccurate inferences about the model parameters.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Mathematical Representation</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Let $ e_i $ be the residual for the $ i $-th observation. The normality assumption can be represented as:\n",
    "</p>\n",
    "<p style=\"text-align: center; font-size: 16px;\">\n",
    "$$ e_i \\sim N(0, \\sigma^2) $$\n",
    "</p>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "This means that the residuals are assumed to follow a normal distribution with a mean of 0 and a constant variance $ \\sigma^2 $.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Summary</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Ensuring the normality of residuals is crucial for the validity of statistical tests and confidence intervals in linear regression. Detecting and addressing non-normality helps maintain the accuracy of the model's inferences.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">How to Address Non-Normality</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "If you detect non-normality, consider these approaches:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Transforming Variables:</strong> Apply transformations (e.g., log, square root) to the dependent variable to achieve normality.</li>\n",
    "<li><strong>Removing Outliers:</strong> Identify and remove outliers that cause deviations from normality.</li>\n",
    "<li><strong>Using Non-Parametric Methods:</strong> Consider using non-parametric regression methods that do not assume normality.</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65feafbc",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #4b0082; padding: 15px; border-radius: 10px; background-color: #f9f9f9;\">\n",
    "\n",
    "<h1 style=\"color: #4b0082;\">5.Multicollinearity</h1>\n",
    "\n",
    "<p style=\"font-size: 16px; color: #333;\">\n",
    "The multicollinearity assumption in linear regression states that the independent variables should not be highly correlated with each other. High multicollinearity occurs when two or more independent variables are highly linearly related, which can lead to difficulties in estimating separate effects of each variable on the dependent variable.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Context and Importance</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "When multicollinearity is present, the variance of the regression coefficients increases, making the estimates very sensitive to changes in the model. This can lead to unreliable estimates, where small changes in the data can result in large changes in the coefficients. It can also make it difficult to assess the individual impact of each independent variable.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Checking for Multicollinearity</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "To check for multicollinearity, consider the following methods:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Correlation Matrix:</strong> Calculate the pairwise correlations between independent variables. High correlation coefficients (close to 1 or -1) indicate potential multicollinearity.</li>\n",
    "<li><strong>Variance Inflation Factor (VIF):</strong> VIF measures how much the variance of an estimated regression coefficient increases due to multicollinearity. A VIF value greater than 10 is often considered indicative of high multicollinearity.</li>\n",
    "<li><strong>Eigenvalues and Condition Index:</strong> Low eigenvalues and a high condition index (above 30) can signal multicollinearity issues.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Addressing Multicollinearity</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "If you detect multicollinearity, consider these approaches to address it:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Removing Highly Correlated Predictors:</strong> Eliminate one of the highly correlated variables to reduce multicollinearity.</li>\n",
    "<li><strong>Combining Variables:</strong> Create a new variable that combines the correlated variables, such as their sum or average.</li>\n",
    "<li><strong>Principal Component Analysis (PCA):</strong> Use PCA to transform the correlated variables into a set of uncorrelated components.</li>\n",
    "<li><strong>Regularization Techniques:</strong> Methods such as Ridge Regression and Lasso Regression can help mitigate the effects of multicollinearity by adding a penalty to the regression coefficients.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"color: #4b0082;\">Example of Correlation Matrix</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Below is an example of a correlation matrix showing high correlations between some of the independent variables:\n",
    "</p>\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"https://cdn-dfnaj.nitrocdn.com/xxeFXDnBIOflfPsgwjDLywIQwPChAOzV/assets/images/optimized/rev-8284464/www.displayr.com/wp-content/uploads/2018/07/rsz_correlation_matrix_3.png\" alt=\"Correlation Matrix Example\" width=\"60%\" style=\"border: 1px solid #ccc;\">\n",
    "</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43cb263",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid #000000; padding: 15px; border-radius: 10px; background-color: #f9f9f9;\">\n",
    "<h2 style=\"color: #800080;\">Multicollinearity in Linear Regression</h2>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Multicollinearity</h3>\n",
    "<p style=\"font-size: 16px; color: #333;\">\n",
    "Multicollinearity occurs when two or more predictor variables in a regression model are highly correlated, meaning that one predictor variable can be linearly predicted from the others with a substantial degree of accuracy. This can cause problems in estimating the coefficients of the regression model.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Why Multicollinearity is Important</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Multicollinearity is important because it can inflate the variances of the parameter estimates, making the estimates very sensitive to changes in the model. This can lead to difficulties in determining the effect of each predictor on the dependent variable and can result in unreliable statistical inferences.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Checking for Multicollinearity</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "To check for multicollinearity, you can use:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Variance Inflation Factor (VIF):</strong> VIF measures how much the variance of a regression coefficient is inflated due to multicollinearity. A VIF value greater than 10 is often considered indicative of high multicollinearity.</li>\n",
    "<li><strong>Correlation Matrix:</strong> A matrix of correlation coefficients can help identify pairs of highly correlated predictors. Correlations above 0.8 or below -0.8 suggest potential multicollinearity.</li>\n",
    "</ul>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Non-Multicollinearity</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "When there is no multicollinearity, the predictor variables are not highly correlated with each other. This ensures that the parameter estimates are stable and that the individual effect of each predictor variable can be accurately determined.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Mathematical Representation</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Let $X_1$, $X_2$, $X_3$, and $X_4$ be predictor variables. Multicollinearity occurs when:\n",
    "</p>\n",
    "<p style=\"text-align: center; font-size: 16px;\">\n",
    "$$ X_1 = aX_2 + bX_3 + cX_4 + d $$\n",
    "</p>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "for some constants $a$, $b$, $c$, and $d$. This indicates a linear relationship between $X_1$ and a combination of $X_2$, $X_3$, and $X_4$. A high VIF indicates multicollinearity. By a rule of thumbs vIF higher than 10 is considered high. For the example above the VIF is $+\\infty$\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">Summary</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "Multicollinearity can lead to inflated standard errors and unreliable parameter estimates in linear regression. Detecting and addressing multicollinearity helps ensure the stability and reliability of the model.\n",
    "</p>\n",
    "\n",
    "<h3 style=\"color: #800080;\">How to Address Multicollinearity</h3>\n",
    "<p style=\"font-size: 14px; color: #333;\">\n",
    "If you detect multicollinearity, consider these approaches:\n",
    "</p>\n",
    "<ul style=\"font-size: 14px; color: #333;\">\n",
    "<li><strong>Removing Variables:</strong> Remove one of the highly correlated predictor variables from the model.</li>\n",
    "<li><strong>Combining Variables:</strong> Combine highly correlated variables into a single predictor through techniques like principal component analysis (PCA).</li>\n",
    "<li><strong>Regularization Techniques:</strong> Use regularization methods like Ridge or Lasso regression to penalize the coefficients of the correlated predictors.</li>\n",
    "</ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9a164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
